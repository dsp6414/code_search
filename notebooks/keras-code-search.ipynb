{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Text In From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_code rows: 1,426,330\n",
      "holdout_code rows: 75,071\n",
      "total code rows: 1,501,401\n",
      "\n",
      "train_comment rows: 1,426,330\n",
      "holdout_comment rows: 75,071\n",
      "total comment rows: 1,501,401\n"
     ]
    }
   ],
   "source": [
    "# with open('/ds/hohsiangwu/rosetta/Python/train.function', 'r') as f:\n",
    "#     t_code = f.readlines()\n",
    "\n",
    "# with open('/ds/hohsiangwu/rosetta/Python/valid.function', 'r') as f:\n",
    "#     v_code = f.readlines()\n",
    "\n",
    "# with open('/ds/hohsiangwu/rosetta/Python/test.function', 'r') as f:\n",
    "#     holdout_code = f.readlines()\n",
    "\n",
    "# train_code = t_code + v_code\n",
    "\n",
    "# print(f'train_code rows: {len(train_code):,}')\n",
    "# print(f'holdout_code rows: {len(holdout_code):,}')\n",
    "# print(f'total code rows: {len(holdout_code + train_code):,}')\n",
    "\n",
    "\n",
    "# with open('/ds/hohsiangwu/rosetta/Python/train.docstring', 'r') as f:\n",
    "#     t_comment = f.readlines()\n",
    "\n",
    "# with open('/ds/hohsiangwu/rosetta/Python/valid.docstring', 'r') as f:\n",
    "#     v_comment = f.readlines()\n",
    "\n",
    "# with open('/ds/hohsiangwu/rosetta/Python/test.docstring', 'r') as f:\n",
    "#     holdout_comment = f.readlines()\n",
    "\n",
    "# train_comment = t_comment + v_comment\n",
    "\n",
    "# print(f'\\ntrain_comment rows: {len(train_comment):,}')\n",
    "# print(f'holdout_comment rows: {len(holdout_comment):,}')\n",
    "# print(f'total comment rows: {len(holdout_comment + train_comment):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 55 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 178 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 50 sec\n",
      "WARNING:root:Finished parsing 1,426,330 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 37 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 15 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 57 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 23 sec\n",
      "WARNING:root:Finished parsing 1,426,330 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 32 sec\n"
     ]
    }
   ],
   "source": [
    "# from ktext.preprocess import processor\n",
    "# code_proc = processor(hueristic_pct_padding=.7, keep_n=10000)\n",
    "# t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "# comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=8000, padding ='post')\n",
    "# t_comment = comment_proc.fit_transform(train_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dill as dpickle\n",
    "# import numpy as np\n",
    "\n",
    "# # Save the preprocessor\n",
    "# with open(PATH/'py_code_proc.dpkl', 'wb') as f:\n",
    "#     dpickle.dump(code_proc, f)\n",
    "\n",
    "# with open(PATH/'py_comment_proc.dpkl', 'wb') as f:\n",
    "#     dpickle.dump(comment_proc, f)\n",
    "\n",
    "# # Save the processed data\n",
    "# np.save(PATH/'py_t_code_vecs.npy', t_code)\n",
    "# np.save(PATH/'py_t_comment_vecs.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrange data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (4978625, 55)\n",
      "Shape of decoder input: (4978625, 14)\n",
      "Shape of decoder target: (4978625, 14)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "\n",
    "encoder_input_data, doc_length = load_encoder_inputs(PATH/'py_t_code_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs(PATH/'py_t_comment_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for /ds/hamel/CodeML/Get_Python_From_BigQuery/py_code_proc.dpkl: 20,002\n",
      "Size of vocabulary for /ds/hamel/CodeML/Get_Python_From_BigQuery/py_comment_proc.dpkl: 15,002\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor(PATH/'py_code_proc.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor(PATH/'py_comment_proc.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Seq2Seq Model For Summarizing Code - (This Is Only For Transfer Learning)\n",
    "\n",
    "Will reuse this for the code search task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 800\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "# x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "# x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU', dropout=.5)(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU', dropout=.5)\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    12001600    Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 55)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 800)          19847200    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 800),  3842400     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 800)    3200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 15002)  12016602    Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 47,714,202\n",
      "Trainable params: 47,709,402\n",
      "Non-trainable params: 4,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4381190 samples, validate on 597435 samples\n",
      "Epoch 1/60\n",
      "4381190/4381190 [==============================] - 1969s 449us/step - loss: 1.9850 - val_loss: 1.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model_2/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 800) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/60\n",
      "4381190/4381190 [==============================] - 1976s 451us/step - loss: 1.9235 - val_loss: 1.9470\n",
      "Epoch 3/60\n",
      "4381190/4381190 [==============================] - 1983s 453us/step - loss: 1.8917 - val_loss: 1.9235\n",
      "Epoch 4/60\n",
      "4381190/4381190 [==============================] - 1979s 452us/step - loss: 1.8578 - val_loss: 1.9043\n",
      "Epoch 5/60\n",
      "4381190/4381190 [==============================] - 1985s 453us/step - loss: 1.8345 - val_loss: 1.8900\n",
      "Epoch 6/60\n",
      "4381190/4381190 [==============================] - 1983s 453us/step - loss: 1.8143 - val_loss: 1.8767\n",
      "Epoch 7/60\n",
      "4381190/4381190 [==============================] - 1993s 455us/step - loss: 1.7969 - val_loss: 1.8668\n",
      "Epoch 8/60\n",
      "4381190/4381190 [==============================] - 1996s 456us/step - loss: 1.7849 - val_loss: 1.8539\n",
      "Epoch 9/60\n",
      "4381190/4381190 [==============================] - 1984s 453us/step - loss: 1.7689 - val_loss: 1.8465\n",
      "Epoch 10/60\n",
      "4381190/4381190 [==============================] - 2008s 458us/step - loss: 1.7578 - val_loss: 1.8401\n",
      "Epoch 11/60\n",
      "4381190/4381190 [==============================] - 2002s 457us/step - loss: 1.7473 - val_loss: 1.8351\n",
      "Epoch 12/60\n",
      "4381190/4381190 [==============================] - 2015s 460us/step - loss: 1.7383 - val_loss: 1.8472\n",
      "Epoch 13/60\n",
      "4381190/4381190 [==============================] - 2002s 457us/step - loss: 1.7300 - val_loss: 1.8236\n",
      "Epoch 14/60\n",
      "4381190/4381190 [==============================] - 2018s 461us/step - loss: 1.7217 - val_loss: 1.8191\n",
      "Epoch 15/60\n",
      "4381190/4381190 [==============================] - 2043s 466us/step - loss: 1.7148 - val_loss: 1.8137\n",
      "Epoch 16/60\n",
      "4381190/4381190 [==============================] - 2044s 467us/step - loss: 1.7080 - val_loss: 1.8103\n",
      "Epoch 17/60\n",
      "4381190/4381190 [==============================] - 2053s 469us/step - loss: 1.7020 - val_loss: 1.8070\n",
      "Epoch 18/60\n",
      "4381190/4381190 [==============================] - 2042s 466us/step - loss: 1.6960 - val_loss: 1.8016\n",
      "Epoch 19/60\n",
      "4381190/4381190 [==============================] - 2058s 470us/step - loss: 1.6907 - val_loss: 1.8006\n",
      "Epoch 20/60\n",
      "4381190/4381190 [==============================] - 2043s 466us/step - loss: 1.6857 - val_loss: 1.7960\n",
      "Epoch 21/60\n",
      "4381190/4381190 [==============================] - 2063s 471us/step - loss: 1.6832 - val_loss: 1.7925\n",
      "Epoch 22/60\n",
      "4381190/4381190 [==============================] - 2051s 468us/step - loss: 1.6864 - val_loss: 1.7909\n",
      "Epoch 23/60\n",
      "4381190/4381190 [==============================] - 2076s 474us/step - loss: 1.6733 - val_loss: 1.7890\n",
      "Epoch 24/60\n",
      "4381190/4381190 [==============================] - 2074s 473us/step - loss: 1.6741 - val_loss: 1.8018\n",
      "Epoch 25/60\n",
      "4381190/4381190 [==============================] - 2030s 463us/step - loss: 2.2771 - val_loss: 2.6740\n",
      "Epoch 26/60\n",
      "4381190/4381190 [==============================] - 2006s 458us/step - loss: 2.6119 - val_loss: 2.5492\n",
      "Epoch 27/60\n",
      "4381190/4381190 [==============================] - 2045s 467us/step - loss: 2.4264 - val_loss: 2.2413\n",
      "Epoch 28/60\n",
      "4381190/4381190 [==============================] - 2073s 473us/step - loss: 2.1019 - val_loss: 2.0287\n",
      "Epoch 29/60\n",
      "4381190/4381190 [==============================] - 2071s 473us/step - loss: 1.9131 - val_loss: 1.9264\n",
      "Epoch 30/60\n",
      "4381190/4381190 [==============================] - 2082s 475us/step - loss: 1.8188 - val_loss: 1.8816\n",
      "Epoch 31/60\n",
      "4381190/4381190 [==============================] - 2081s 475us/step - loss: 1.7784 - val_loss: 1.8604\n",
      "Epoch 32/60\n",
      "4381190/4381190 [==============================] - 2076s 474us/step - loss: 1.7540 - val_loss: 1.8461\n",
      "Epoch 33/60\n",
      "4381190/4381190 [==============================] - 2072s 473us/step - loss: 1.7375 - val_loss: 1.8334\n",
      "Epoch 34/60\n",
      "4381190/4381190 [==============================] - 2070s 472us/step - loss: 1.7249 - val_loss: 1.8281\n",
      "Epoch 35/60\n",
      "4381190/4381190 [==============================] - 2072s 473us/step - loss: 1.7147 - val_loss: 1.8227\n",
      "Epoch 36/60\n",
      "4381190/4381190 [==============================] - 2064s 471us/step - loss: 1.7063 - val_loss: 1.8153\n",
      "Epoch 37/60\n",
      "4381190/4381190 [==============================] - 2063s 471us/step - loss: 1.6986 - val_loss: 1.8096\n",
      "Epoch 38/60\n",
      "4381190/4381190 [==============================] - 2064s 471us/step - loss: 1.6938 - val_loss: 1.8050\n",
      "Epoch 39/60\n",
      "4381190/4381190 [==============================] - 2065s 471us/step - loss: 1.6866 - val_loss: 1.8028\n",
      "Epoch 40/60\n",
      "4381190/4381190 [==============================] - 2063s 471us/step - loss: 1.6815 - val_loss: 1.7992\n",
      "Epoch 41/60\n",
      "4381190/4381190 [==============================] - 2065s 471us/step - loss: 1.6775 - val_loss: 1.7952\n",
      "Epoch 42/60\n",
      "4381190/4381190 [==============================] - 2063s 471us/step - loss: 1.6722 - val_loss: 1.7935\n",
      "Epoch 43/60\n",
      "4381190/4381190 [==============================] - 2059s 470us/step - loss: 1.6688 - val_loss: 1.7899\n",
      "Epoch 44/60\n",
      "4381190/4381190 [==============================] - 2056s 469us/step - loss: 1.6646 - val_loss: 1.7893\n",
      "Epoch 45/60\n",
      "4381190/4381190 [==============================] - 2054s 469us/step - loss: 1.6607 - val_loss: 1.7889\n",
      "Epoch 46/60\n",
      "4381190/4381190 [==============================] - 2055s 469us/step - loss: 1.6583 - val_loss: 1.7834\n",
      "Epoch 47/60\n",
      "4381190/4381190 [==============================] - 2073s 473us/step - loss: 1.6552 - val_loss: 1.7829\n",
      "Epoch 48/60\n",
      "4381190/4381190 [==============================] - 2063s 471us/step - loss: 1.6531 - val_loss: 1.7830\n",
      "Epoch 49/60\n",
      "4381190/4381190 [==============================] - 2060s 470us/step - loss: 1.6492 - val_loss: 1.8324\n",
      "Epoch 50/60\n",
      "4381190/4381190 [==============================] - 2057s 470us/step - loss: 1.6469 - val_loss: 1.7792\n",
      "Epoch 51/60\n",
      "4381190/4381190 [==============================] - 2074s 473us/step - loss: 1.6510 - val_loss: 1.7782\n",
      "Epoch 52/60\n",
      "4381190/4381190 [==============================] - 2069s 472us/step - loss: 1.6429 - val_loss: 1.7774\n",
      "Epoch 53/60\n",
      "4381190/4381190 [==============================] - 2065s 471us/step - loss: 1.6399 - val_loss: 1.7732\n",
      "Epoch 54/60\n",
      "4381190/4381190 [==============================] - 2066s 471us/step - loss: 1.6877 - val_loss: 1.7833\n",
      "Epoch 55/60\n",
      "4381190/4381190 [==============================] - 2066s 472us/step - loss: 1.6577 - val_loss: 1.7757\n",
      "Epoch 56/60\n",
      "4381190/4381190 [==============================] - 2055s 469us/step - loss: 1.6407 - val_loss: 1.7769\n",
      "Epoch 57/60\n",
      "4381190/4381190 [==============================] - 2044s 467us/step - loss: 1.6381 - val_loss: 1.7743\n",
      "Epoch 58/60\n",
      "4381190/4381190 [==============================] - 2034s 464us/step - loss: 1.7265 - val_loss: 1.7847\n",
      "Epoch 59/60\n",
      "4381190/4381190 [==============================] - 2055s 469us/step - loss: 1.6435 - val_loss: 1.7747\n",
      "Epoch 60/60\n",
      "4381190/4381190 [==============================] - 2055s 469us/step - loss: 1.6376 - val_loss: 1.7740\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "import numpy as np\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.0015), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "script_name_base = 'py_func_sum_v2_'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 900\n",
    "epochs = 60\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/topology.py:2379: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model_2/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 800) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery')\n",
    "#seq2seq_Model.save(PATH/'seq2seq_code_search_py_v2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If notebook dies can restart from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for /ds/hamel/CodeML/Get_Python_From_BigQuery/py_code_proc.dpkl: 20,002\n",
      "Size of vocabulary for /ds/hamel/CodeML/Get_Python_From_BigQuery/py_comment_proc.dpkl: 15,002\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from seq2seq_utils import load_text_processor\n",
    "from pathlib import Path\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery')\n",
    "\n",
    "num_encoder_tokens, body_pp = load_text_processor(PATH/'py_code_proc.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor(PATH/'py_comment_proc.dpkl')\n",
    "seq2seq_Model = load_model(PATH/'seq2seq_code_search_py_v2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 15635 =================\n",
      "\n",
      "Issue Body:\n",
      " app route login methods GET POST def login if g user return redirect url_for timeline error None if request method POST user query_db select from user where username request form username one True if user is None error Invalid username elif not check_password_hash user pw_hash request form password error Invalid password else flash You were logged in session user_id user user_id return redirect url_for timeline return render_template login html error error\n",
      " \n",
      "\n",
      "Original Title:\n",
      " logs the user in .\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " logs the user in\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 21181 =================\n",
      "\n",
      "Issue Body:\n",
      " def make_user_from_json user_dict user User int user_dict age str user_dict sex user_dict hobbies user_dict userType user_dict lovedCategories user_dict alreadyGifted return user\n",
      " \n",
      "\n",
      "Original Title:\n",
      " generate user : return : user\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " makes a dictionary from a user object\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 5176 =================\n",
      "\n",
      "Issue Body:\n",
      " def most_similar_to_vector vec number 20 sim self assoc dot vec sorted np argsort sim 1 selected sorted 0 number return self labels sel sim sel for sel in selected\n",
      " \n",
      "\n",
      "Original Title:\n",
      " finds the term most similar to the given vector , returning a ( term , similarity ) tuple . this method is much faster than terms_similar_to_vector .\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " return the most similar to the most similar to the given vector\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 17277 =================\n",
      "\n",
      "Issue Body:\n",
      " def save self path None path path or self _workbook get_path if not path raise Exception Please specify a filename for the XMind file path utils get_abs_path path file_name ext utils split_ext path if ext const XMIND_EXT raise Exception XMind filenames require a s extension const XMIND_EXT content self _get_content f utils compress path f write content const CONTENT_XML\n",
      " \n",
      "\n",
      "Original Title:\n",
      " save the workbook to the given path . if the path is not given , then will save to the path set in workbook .\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " save the contents of the workbook to the file system\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 11961 =================\n",
      "\n",
      "Issue Body:\n",
      " def update self try self _state self smartplug state if self smartplug has_emeter emeter_readings self smartplug get_emeter_realtime self _emeter_params ATTR_CURRENT_CONSUMPTION 1f W emeter_readings power self _emeter_params ATTR_TOTAL_CONSUMPTION 2f kW emeter_readings total self _emeter_params ATTR_VOLTAGE 2f V emeter_readings voltage self _emeter_params ATTR_CURRENT 1f A emeter_readings current emeter_statics self smartplug get_emeter_daily try self _emeter_params ATTR_DAILY_CONSUMPTION 2f kW emeter_statics int time strftime e except KeyError pass except OSError _LOGGER warning Could not update status for s self name\n",
      " \n",
      "\n",
      "Original Title:\n",
      " update the tp - link switch 's state .\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " update the tp link switch s state\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "PATH = Path('/ds/hohsiangwu/projects/semantic_search')\n",
    "\n",
    "with open(PATH/'test.function', 'r') as f:\n",
    "    holdout_code = f.readlines()\n",
    "\n",
    "with open(PATH/'test.docstring', 'r') as f:\n",
    "    holdout_comment = f.readlines()\n",
    "\n",
    "\n",
    "demo_testdf = pd.DataFrame({'body':holdout_code, 'issue_title':holdout_comment, 'issue_url':''})\n",
    "seq2seq_inf.demo_model_predictions(n=5, issue_df=demo_testdf)\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tune This To Predict Universal Sentence Encoder Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    def get_embeddings(text_blob_list):\n",
    "            emb = session.run(embed(text_blob_list))\n",
    "            return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_embeddings(train_comment[:1000])\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import chunked\n",
    "train_chunked = list(chunked(train_comment, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "use_emb_list = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    for x in train_chunked:\n",
    "        print(i)\n",
    "        i+=1\n",
    "        #75th percentile of comment length is 89 characters, so I made limit 200.\n",
    "        use_emb_list.append(get_embeddings([s[:200] for s in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_emb = np.concatenate(use_emb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding !\n",
    "# np.save('use_emb.npy', use_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Mapping Function To Sentence Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for /ds/hamel/CodeML/Get_Python_From_BigQuery/py_code_proc.dpkl: 20,002\n",
      "Size of vocabulary for /ds/hamel/CodeML/Get_Python_From_BigQuery/py_comment_proc.dpkl: 15,002\n",
      "Shape of encoder input: (4978625, 55)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import numpy as np\n",
    "from seq2seq_utils import extract_encoder_model, load_encoder_inputs\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "from keras.models import load_model, Model\n",
    "from seq2seq_utils import load_text_processor\n",
    "\n",
    "from pathlib import Path\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery')\n",
    "\n",
    "num_encoder_tokens, body_pp = load_text_processor(PATH/'py_code_proc.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor(PATH/'py_comment_proc.dpkl')\n",
    "encoder_input_data, doc_length = load_encoder_inputs(PATH/'py_t_code_vecs.npy')\n",
    "seq2seq_Model = load_model(PATH/'seq2seq_code_search_py_v2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fitlam Embeddings\n",
    "fastailm_emb = np.load(PATH/'combined_fastailm_emb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978625, 1200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastailm_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978625, 55)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder-Input (InputLayer)   (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "Body-Word-Embedding (Embeddi (None, 55, 800)           16001600  \n",
      "_________________________________________________________________\n",
      "Encoder-Batchnorm-1 (BatchNo (None, 55, 800)           3200      \n",
      "_________________________________________________________________\n",
      "Encoder-Last-GRU (GRU)       [(None, 800), (None, 800) 3842400   \n",
      "=================================================================\n",
      "Total params: 19,847,200\n",
      "Trainable params: 0\n",
      "Non-trainable params: 19,847,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Extract Encoder from seq2seq model\n",
    "encoder_model = extract_encoder_model(seq2seq_Model)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7f40c798f9b0> False\n",
      "<keras.layers.embeddings.Embedding object at 0x7f40c798fac8> False\n",
      "<keras.layers.normalization.BatchNormalization object at 0x7f40c798fb70> False\n",
      "<keras.layers.recurrent.GRU object at 0x7f40c798fcf8> False\n"
     ]
    }
   ],
   "source": [
    "# Freeze Encoder Model\n",
    "for l in encoder_model.layers:\n",
    "    l.trainable = False\n",
    "    print(l, l.trainable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct New Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, -0.9999999, 0.10509811)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Figure out range of Sentence Encoder\n",
    "fastailm_emb.max(), fastailm_emb.min(), fastailm_emb.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#### Encoder Model ####\n",
    "\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "enc_out = encoder_model(encoder_inputs)\n",
    "\n",
    "# first dense layer with batch norm\n",
    "fc = Dense(1024, activation='relu')(enc_out)\n",
    "fc = BatchNormalization(name='bn-1')(fc)\n",
    "\n",
    "# force network to bound output between (.152, -.152) to match target range\n",
    "fc = Dense(1200, activation='tanh')(fc)\n",
    "#target_emb = Lambda(lambda x: x * .152)(fc)\n",
    "\n",
    "codeSearch_Model = Model([encoder_inputs], fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder-Input (InputLayer)   (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "Encoder-Model (Model)        (None, 800)               19847200  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              820224    \n",
      "_________________________________________________________________\n",
      "bn-1 (BatchNormalization)    (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1200)              1230000   \n",
      "=================================================================\n",
      "Total params: 21,901,520\n",
      "Trainable params: 2,052,272\n",
      "Non-trainable params: 19,849,248\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "codeSearch_Model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4381190 samples, validate on 597435 samples\n",
      "Epoch 1/50\n",
      "4381190/4381190 [==============================] - 286s 65us/step - loss: -0.8031 - val_loss: -0.7955\n",
      "Epoch 2/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8048 - val_loss: -0.8006\n",
      "Epoch 3/50\n",
      "4381190/4381190 [==============================] - 285s 65us/step - loss: -0.8050 - val_loss: -0.8014\n",
      "Epoch 4/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8053 - val_loss: -0.8000\n",
      "Epoch 5/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8055 - val_loss: -0.8028\n",
      "Epoch 6/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8000 - val_loss: -0.8031\n",
      "Epoch 7/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8059 - val_loss: -0.8037\n",
      "Epoch 8/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8061 - val_loss: -0.8034\n",
      "Epoch 9/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8062 - val_loss: -0.8035\n",
      "Epoch 10/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8063 - val_loss: -0.8046\n",
      "Epoch 11/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8064 - val_loss: -0.8036\n",
      "Epoch 12/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8065 - val_loss: -0.8045\n",
      "Epoch 13/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8066 - val_loss: -0.8050\n",
      "Epoch 14/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8050 - val_loss: -0.8042\n",
      "Epoch 15/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8069 - val_loss: -0.8042\n",
      "Epoch 16/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8069 - val_loss: -0.8046\n",
      "Epoch 17/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8070 - val_loss: -0.8048\n",
      "Epoch 18/50\n",
      "4381190/4381190 [==============================] - 291s 67us/step - loss: -0.8070 - val_loss: -0.8040\n",
      "Epoch 19/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8071 - val_loss: -0.8046\n",
      "Epoch 20/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8072 - val_loss: -0.8047\n",
      "Epoch 21/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8057 - val_loss: -0.8055\n",
      "Epoch 22/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8073 - val_loss: -0.8056\n",
      "Epoch 23/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8073 - val_loss: -0.8038\n",
      "Epoch 24/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8073 - val_loss: -0.8035\n",
      "Epoch 25/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8074 - val_loss: -0.8048\n",
      "Epoch 26/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8074 - val_loss: -0.8046\n",
      "Epoch 27/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8075 - val_loss: -0.8048\n",
      "Epoch 28/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8058 - val_loss: -0.7861\n",
      "Epoch 29/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8075 - val_loss: -0.8049\n",
      "Epoch 30/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8076 - val_loss: -0.8056\n",
      "Epoch 31/50\n",
      "4381190/4381190 [==============================] - 285s 65us/step - loss: -0.8076 - val_loss: -0.8054\n",
      "Epoch 32/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8076 - val_loss: -0.8043\n",
      "Epoch 33/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8076 - val_loss: -0.8050\n",
      "Epoch 34/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8076 - val_loss: -0.8054\n",
      "Epoch 35/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8077 - val_loss: -0.8051\n",
      "Epoch 36/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8064 - val_loss: -0.8045\n",
      "Epoch 37/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8078 - val_loss: -0.8053\n",
      "Epoch 38/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8078 - val_loss: -0.8060\n",
      "Epoch 39/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8078 - val_loss: -0.8059\n",
      "Epoch 40/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8078 - val_loss: -0.8057\n",
      "Epoch 41/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8078 - val_loss: -0.8051\n",
      "Epoch 42/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8078 - val_loss: -0.8045\n",
      "Epoch 43/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8079 - val_loss: -0.8055\n",
      "Epoch 44/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8069 - val_loss: -0.8065\n",
      "Epoch 45/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8079 - val_loss: -0.8054\n",
      "Epoch 46/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8079 - val_loss: -0.8045\n",
      "Epoch 47/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8079 - val_loss: -0.8057\n",
      "Epoch 48/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8080 - val_loss: -0.8050\n",
      "Epoch 49/50\n",
      "4381190/4381190 [==============================] - 284s 65us/step - loss: -0.8080 - val_loss: -0.8057\n",
      "Epoch 50/50\n",
      "4381190/4381190 [==============================] - 291s 66us/step - loss: -0.8080 - val_loss: -0.8061\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "from keras import optimizers\n",
    "\n",
    "codeSearch_Model.compile(optimizer=optimizers.Nadam(lr=0.005), loss='cosine_proximity')\n",
    "script_name_base = 'cs_model_fitlam_'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 5000\n",
    "epochs = 50\n",
    "history = codeSearch_Model.fit([encoder_input_data], fastailm_emb,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#codeSearch_Model.save(PATH/'codeSearch_Model_frozen.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/ds/hamel/CodeML/Get_Python_From_BigQuery')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "codeSearch_Model = load_model(PATH/'codeSearch_Model_frozen.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Encoder-Input (InputLayer)   (None, 55)                0         \n",
      "_________________________________________________________________\n",
      "Encoder-Model (Model)        (None, 800)               19847200  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              820224    \n",
      "_________________________________________________________________\n",
      "bn-1 (BatchNormalization)    (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1200)              1230000   \n",
      "=================================================================\n",
      "Total params: 21,901,520\n",
      "Trainable params: 2,052,272\n",
      "Non-trainable params: 19,849,248\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "codeSearch_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Viability of Lookups In This Vectorspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize all the code!\n",
    "batch_size = 1300\n",
    "train_vecs = codeSearch_Model.predict(encoder_input_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(PATH/'train_code_to_lang_vecs.npy', train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((75071, 512), (1426330, 512))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load vectorized code\n",
    "train_vecs = np.load(PATH/'train_code_to_lang_vecs.npy')\n",
    "train_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_eng_query(text):\n",
    "    return get_embeddings([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "index.addDataPointBatch(train_vecs)\n",
    "index.createIndex({'post': 2}, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_code = t_code + v_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vec_eng_query('Change colors on web page.')\n",
    "ids, distances = index.knnQuery(test, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def set_bgcolor self bgcolor assert bgcolor startswith color gtk gdk color_parse bgcolor self modify_bg gtk STATE_NORMAL color\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ref_code[ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = vec_eng_query('aggregate and sort data.')\n",
    "ids, distances = index.knnQuery(test, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def stats self stats self _stats if stats is None db current db s3db current s3db utable s3db auth_user otable s3db org_organisation left otable on otable id utable organisation_id query utable deleted False users db query select otable root_organisation utable registration_key utable timestmp left left import datetime now current request utcnow start now datetime timedelta days 30 replace hour 0 minute 0 second 0 microsecond 0 stats for user in users account user auth_user organisation user org_organisation root_org organisation root_organisation if not root_org continue if root_org in stats org_stats stats root_org else org_stats stats root_org 0 0 0 org_stats 1 if account registration_key org_stats 1 timestmp account timestmp if timestmp and timestmp start org_stats 1 self _stats stats return stats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ref_code[ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1283697"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
