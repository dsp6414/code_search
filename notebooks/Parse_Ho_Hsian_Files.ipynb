{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Ho-Hsian's Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "PATH = Path('/ds/hohsiangwu/projects/semantic_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1.post2'"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python_Code_Search.ipynb\t     py_func_sum.epoch01-val2.21853.hdf5\r\n",
      "__pycache__\t\t\t     py_func_sum.epoch02-val1.91378.hdf5\r\n",
      "codeSearch_Model.hdf5\t\t     py_func_sum.epoch02-val1.97722.hdf5\r\n",
      "codeSearch_Model_20.hdf5\t     py_func_sum.epoch02-val2.13045.hdf5\r\n",
      "core\t\t\t\t     py_func_sum.epoch02-val2.13085.hdf5\r\n",
      "cs_model_.epoch01-val-0.77763.hdf5   py_func_sum.epoch03-val1.93079.hdf5\r\n",
      "cs_model_.epoch01-val-0.77858.hdf5   py_func_sum.epoch03-val1.97410.hdf5\r\n",
      "cs_model_.epoch01-val-0.77885.hdf5   py_func_sum.epoch03-val2.08514.hdf5\r\n",
      "cs_model_.epoch01-val-0.77928.hdf5   py_func_sum.epoch03-val2.08926.hdf5\r\n",
      "cs_model_.epoch02-val-0.78204.hdf5   py_func_sum.epoch04-val1.91135.hdf5\r\n",
      "cs_model_.epoch02-val-0.78222.hdf5   py_func_sum.epoch04-val1.92982.hdf5\r\n",
      "cs_model_.epoch02-val-0.78283.hdf5   py_func_sum.epoch04-val1.97054.hdf5\r\n",
      "cs_model_.epoch02-val-0.78312.hdf5   py_func_sum.epoch04-val2.05687.hdf5\r\n",
      "cs_model_.epoch03-val-0.78547.hdf5   py_func_sum.epoch04-val2.06309.hdf5\r\n",
      "cs_model_.epoch03-val-0.78561.hdf5   py_func_sum.epoch05-val1.92685.hdf5\r\n",
      "cs_model_.epoch03-val-0.78583.hdf5   py_func_sum.epoch05-val1.96681.hdf5\r\n",
      "cs_model_.epoch03-val-0.78590.hdf5   py_func_sum.epoch05-val2.03806.hdf5\r\n",
      "cs_model_.epoch04-val-0.78691.hdf5   py_func_sum.epoch05-val2.04291.hdf5\r\n",
      "cs_model_.epoch04-val-0.78741.hdf5   py_func_sum.epoch06-val1.91100.hdf5\r\n",
      "cs_model_.epoch04-val-0.78746.hdf5   py_func_sum.epoch06-val1.96212.hdf5\r\n",
      "cs_model_.epoch04-val-0.78750.hdf5   py_func_sum.epoch06-val2.02185.hdf5\r\n",
      "cs_model_.epoch05-val-0.78823.hdf5   py_func_sum.epoch06-val2.02998.hdf5\r\n",
      "cs_model_.epoch05-val-0.78892.hdf5   py_func_sum.epoch07-val1.91082.hdf5\r\n",
      "cs_model_.epoch05-val-0.78897.hdf5   py_func_sum.epoch07-val1.92569.hdf5\r\n",
      "cs_model_.epoch06-val-0.78951.hdf5   py_func_sum.epoch07-val1.96121.hdf5\r\n",
      "cs_model_.epoch06-val-0.78974.hdf5   py_func_sum.epoch07-val2.01462.hdf5\r\n",
      "cs_model_.epoch06-val-0.78983.hdf5   py_func_sum.epoch08-val1.92487.hdf5\r\n",
      "cs_model_.epoch06-val-0.79020.hdf5   py_func_sum.epoch08-val1.95858.hdf5\r\n",
      "cs_model_.epoch07-val-0.79014.hdf5   py_func_sum.epoch08-val2.00655.hdf5\r\n",
      "cs_model_.epoch07-val-0.79024.hdf5   py_func_sum.epoch09-val1.90913.hdf5\r\n",
      "cs_model_.epoch07-val-0.79045.hdf5   py_func_sum.epoch09-val1.92461.hdf5\r\n",
      "cs_model_.epoch08-val-0.79063.hdf5   py_func_sum.epoch09-val1.95447.hdf5\r\n",
      "cs_model_.epoch08-val-0.79074.hdf5   py_func_sum.epoch09-val1.99762.hdf5\r\n",
      "cs_model_.epoch08-val-0.79108.hdf5   py_func_sum.epoch10-val1.92192.hdf5\r\n",
      "cs_model_.epoch09-val-0.79118.hdf5   py_func_sum.epoch10-val1.95290.hdf5\r\n",
      "cs_model_.epoch09-val-0.79131.hdf5   py_func_sum.epoch10-val1.99180.hdf5\r\n",
      "cs_model_.epoch09-val-0.79164.hdf5   py_func_sum.epoch11-val1.90710.hdf5\r\n",
      "cs_model_.epoch10-val-0.79145.hdf5   py_func_sum.epoch11-val1.92150.hdf5\r\n",
      "cs_model_.epoch10-val-0.79178.hdf5   py_func_sum.epoch11-val1.95022.hdf5\r\n",
      "cs_model_.epoch10-val-0.79200.hdf5   py_func_sum.epoch12-val1.92125.hdf5\r\n",
      "cs_model_.epoch11-val-0.79197.hdf5   py_func_sum.epoch12-val1.94859.hdf5\r\n",
      "cs_model_.epoch11-val-0.79203.hdf5   py_func_sum.epoch13-val1.94673.hdf5\r\n",
      "cs_model_.epoch12-val-0.79212.hdf5   py_func_sum.epoch14-val1.92011.hdf5\r\n",
      "cs_model_.epoch12-val-0.79237.hdf5   py_func_sum.epoch14-val1.94534.hdf5\r\n",
      "cs_model_.epoch12-val-0.79261.hdf5   py_func_sum.epoch15-val1.91836.hdf5\r\n",
      "cs_model_.epoch13-val-0.79247.hdf5   py_func_sum.epoch15-val1.94325.hdf5\r\n",
      "cs_model_.epoch13-val-0.79291.hdf5   py_func_sum.epoch16-val1.90449.hdf5\r\n",
      "cs_model_.epoch14-val-0.79251.hdf5   py_func_sum.epoch16-val1.94144.hdf5\r\n",
      "cs_model_.epoch15-val-0.79274.hdf5   py_func_sum.epoch17-val1.91776.hdf5\r\n",
      "cs_model_.epoch15-val-0.79300.hdf5   py_func_sum.epoch17-val1.93795.hdf5\r\n",
      "cs_model_.epoch15-val-0.79315.hdf5   py_func_sum.epoch18-val1.91708.hdf5\r\n",
      "cs_model_.epoch16-val-0.79305.hdf5   py_func_sum.epoch19-val1.91508.hdf5\r\n",
      "cs_model_.epoch16-val-0.79329.hdf5   py_func_sum.epoch20-val1.90426.hdf5\r\n",
      "cs_model_.epoch17-val-0.79287.hdf5   py_func_sum.epoch20-val1.91455.hdf5\r\n",
      "cs_model_.epoch17-val-0.79309.hdf5   py_func_sum.epoch20-val1.93397.hdf5\r\n",
      "cs_model_.epoch17-val-0.79334.hdf5   py_func_sum.epoch22-val1.90381.hdf5\r\n",
      "cs_model_.epoch18-val-0.79321.hdf5   py_func_sum.epoch23-val1.90195.hdf5\r\n",
      "cs_model_.epoch18-val-0.79330.hdf5   py_func_sum.epoch28-val1.90159.hdf5\r\n",
      "cs_model_.epoch18-val-0.79351.hdf5   py_func_sum.log\r\n",
      "cs_model_.epoch19-val-0.79332.hdf5   py_t_code_vecs.npy\r\n",
      "cs_model_.epoch19-val-0.79338.hdf5   py_t_comment_vecs.npy\r\n",
      "cs_model_.epoch20-val-0.79357.hdf5   python_files_df.pkl\r\n",
      "cs_model_.epoch20-val-0.79380.hdf5   repository_scores.pkl\r\n",
      "cs_model_.log\t\t\t     seq2seq_code_search_py.hdf5\r\n",
      "holdout_vecs.npy\t\t     seq2seq_code_search_py_50.hdf5\r\n",
      "keras-code-search.ipynb\t\t     seq2seq_utils.py\r\n",
      "lang-model-code-comments.ipynb\t     test.docstring\r\n",
      "lineage\t\t\t\t     test.function\r\n",
      "nmslib_index\t\t\t     test.lineage\r\n",
      "nmslib_index_ori\t\t     train.docstring\r\n",
      "original_function.pkl\t\t     train.function\r\n",
      "pairs.pkl\t\t\t     train.lineage\r\n",
      "py_code_proc.dpkl\t\t     train_vecs.npy\r\n",
      "py_comment_proc.dpkl\t\t     use_emb.npy\r\n",
      "py_func_sum.epoch01-val1.91407.hdf5  valid.docstring\r\n",
      "py_func_sum.epoch01-val1.93303.hdf5  valid.function\r\n",
      "py_func_sum.epoch01-val1.98297.hdf5  valid.lineage\r\n",
      "py_func_sum.epoch01-val2.21809.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "! ls /ds/hohsiangwu/projects/semantic_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_code rows: 4,978,625\n",
      "holdout_code rows: 50,290\n",
      "total code rows: 5,028,915\n",
      "\n",
      "train_comment rows: 4,978,625\n",
      "holdout_comment rows: 50,290\n",
      "total comment rows: 5,028,915\n"
     ]
    }
   ],
   "source": [
    "with open(PATH/'train.function', 'r') as f:\n",
    "    t_code = f.readlines()\n",
    "\n",
    "with open(PATH/'valid.function', 'r') as f:\n",
    "    v_code = f.readlines()\n",
    "\n",
    "with open(PATH/'test.function', 'r') as f:\n",
    "    holdout_code = f.readlines()\n",
    "\n",
    "train_code = t_code + v_code\n",
    "\n",
    "print(f'train_code rows: {len(train_code):,}')\n",
    "print(f'holdout_code rows: {len(holdout_code):,}')\n",
    "print(f'total code rows: {len(holdout_code + train_code):,}')\n",
    "\n",
    "\n",
    "with open(PATH/'train.docstring', 'r') as f:\n",
    "    t_comment = f.readlines()\n",
    "\n",
    "with open(PATH/'valid.docstring', 'r') as f:\n",
    "    v_comment = f.readlines()\n",
    "\n",
    "with open(PATH/'test.docstring', 'r') as f:\n",
    "    holdout_comment = f.readlines()\n",
    "\n",
    "train_comment = t_comment + v_comment\n",
    "\n",
    "print(f'\\ntrain_comment rows: {len(train_comment):,}')\n",
    "print(f'holdout_comment rows: {len(holdout_comment):,}')\n",
    "print(f'total comment rows: {len(holdout_comment + train_comment):,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 55 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 434 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 159 sec\n",
      "WARNING:root:Finished parsing 4,978,625 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 150 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 15 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 250 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 52 sec\n",
      "WARNING:root:Finished parsing 4,978,625 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 67 sec\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=15000, padding ='post')\n",
    "t_comment = comment_proc.fit_transform(train_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open(PATH/'py_code_proc.dpkl', 'wb') as f:\n",
    "    dpickle.dump(code_proc, f)\n",
    "\n",
    "with open(PATH/'py_comment_proc.dpkl', 'wb') as f:\n",
    "    dpickle.dump(comment_proc, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save(PATH/'py_t_code_vecs.npy', t_code)\n",
    "np.save(PATH/'py_t_comment_vecs.npy', t_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get TF USE Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/global_step:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initialize variable module_2/global_step:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")\n",
    "\n",
    "with tf.Session() as session:\n",
    "    def get_embeddings(text_blob_list):\n",
    "            emb = session.run(embed(text_blob_list))\n",
    "            return emb\n",
    "\n",
    "from more_itertools import chunked\n",
    "train_chunked = list(chunked(train_comment, 500000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "use_emb_list = []\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    for x in train_chunked:\n",
    "        print(i)\n",
    "        i+=1\n",
    "        #75th percentile of comment length is 89 characters, so I made limit 200.\n",
    "        use_emb_list.append(get_embeddings([s[:200] for s in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery/')\n",
    "use_emb = np.concatenate(use_emb_list)\n",
    "\n",
    "#save the embedding !\n",
    "np.save(PATH/'use_emb.npy', use_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978625, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize Fastai Language Model Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_avg_emb = np.load(PATH/'val_avg_emb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_avg_emb = np.load(PATH/'train_avg_emb.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4777470, 400)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_avg_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(201155, 400)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_avg_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_train_avg_emb = np.concatenate([train_avg_emb, val_avg_emb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978625, 400)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_train_avg_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH/'concat_train_avg_emb.npy', concat_train_avg_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_max_emb = np.load(PATH/'val_max_emb.npy')\n",
    "train_max_emb = np.load(PATH/'train_max_emb.npy')\n",
    "\n",
    "val_last_emb = np.load(PATH/'val_last_emb.npy')\n",
    "train_last_emb = np.load(PATH/'train_last_emb.npy')\n",
    "\n",
    "concat_train_max_emb = np.concatenate([train_max_emb, val_max_emb])\n",
    "concat_train_last_emb = np.concatenate([train_last_emb, val_last_emb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH/'concat_train_max_emb.npy', concat_train_max_emb)\n",
    "np.save(PATH/'concat_train_last_emb.npy', concat_train_last_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4978625, 400), (4978625, 400))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_train_max_emb.shape, concat_train_last_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_fastailm_emb = np.concatenate([concat_train_avg_emb, concat_train_max_emb, concat_train_last_emb], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978625, 1200)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_fastailm_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(PATH/'combined_fastailm_emb.npy', combined_fastailm_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Locations\n",
    "\n",
    "### Important Files in `/ds/hohsiangwu/projects/semantic_search`\n",
    "\n",
    "1. `{train, valid, test}.function`:   text file, each line is a tokenized function \n",
    "\n",
    "         - train + valid function rows: 4,978,625\n",
    "         - test function rows: 50,290\n",
    "         - total function rows: 5,028,915\n",
    "\n",
    "\n",
    "2. `{train, valid, test}.docstring`:  text file, each line is a tokenized docstring\n",
    "\n",
    "         - train + valid comment rows: 4,978,625\n",
    "         - test comment rows: 50,290\n",
    "         - total comment rows: 5,028,915\n",
    "\n",
    "\n",
    "### Important Files in `/ds/hamel/CodeML/Get_Python_From_BigQuery/`\n",
    "\n",
    "1.  `use_emb.npy`            :          Google Universal Sentence Encoder.  shape: (4978625, 512)\n",
    "\n",
    "2. `concat_train_avg_emb.npy`:          language model average pooling.     shape: (4978625, 400)\n",
    "\n",
    "3. `concat_train_max_emb.npy`:          language model max pooling.         shape: (4978625, 400)\n",
    "\n",
    "4. `concat_train_last_emb.npy`:         language model last hidden state.   shape: (4978625, 400)\n",
    "\n",
    "5. `combined_fastailm_emb`    :         Horizontal concat of [2, 3, 4].     shape: (4978625, 1200)\n",
    "\n",
    "6.  `codeSearch_Model_frozen.hdf5`:     My best keras model with val_loss = -0.8061 cosine proximity loss.   \n",
    "\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "\n",
    "Encoder-Input (InputLayer)   (None, 55)                0         \n",
    "_________________________________________________________________\n",
    "Encoder-Model (Model)        (None, 800)               19847200  \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 1024)              820224    \n",
    "_________________________________________________________________\n",
    "bn-1 (BatchNormalization)    (None, 1024)              4096      \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 1200)              1230000   \n",
    "\n",
    "\n",
    "Total params: 21,901,520\n",
    "Trainable params: 2,052,272\n",
    "Non-trainable params: 19,849,248\n",
    "```\n",
    "7. `lm_fastai_codecomment_model.pytorch`:  This is the language model trained with fastai, you will have to have the latest version of the fast.ai library\n",
    "\n",
    "8.  `lm_fastai_codecomment_model_state_dict`: This is the state dict of the language model, a more lightweight way of re-instantiating the model's parameters.\n",
    "\n",
    "9.  `fitlam_index.nmslib`:  This is the nmslib index that has all the code after it has been vectorized by the language model.  \n",
    "\n",
    "### Important Notebooks\n",
    "\n",
    "1.  `hamel/CodeML/Get_Python_From_BigQuery/Parse_Ho_Hsian_Files.ipynb`  this notebook where I do things that are CPU intense:\n",
    " - preprocess the {training, validation, lineage} files. \n",
    " - run all the comments through the vectorizers (did a version for both Google and My own languagel model).\n",
    " - Loaded all the vectors into an NMS Lib\n",
    " - **This is the notebook where the actual demo lives**\n",
    "\n",
    "2. `hamel/fastai/courses/dl1/lang-model-code-comments.ipynb`:  this notebook is where i trained the fastai language model, and also where I then used the trained model to vectorize all the comments.\n",
    "\n",
    "3. `/hamel/CodeML/projects/function_summarizer/keras-code-search.ipynb` this is the notebook where I\n",
    " - train a function summarizer \n",
    " - fine tune the function summarizer to predict embedding instead of docstring\n",
    " - make predictions for all the training data to vectorize all the code (happens on GPU).  This is the code that is loaded into the index where you want to do nearest neighbor search from.\n",
    "\n",
    "4. `hamel/CodeML/Get_Python_From_BigQuery/Get%20Data%20For%20Python_Code_Search.ipynb` - this notebook I was using to build my own training set, but I abandoned this in favor of Ho-Hsiang's Training dataset which he gave me."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place Embeddings Into NMSLIB Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_use_index = nmslib.init(method='hnsw', space='cosinesimil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_use_index.loadIndex(f'{strPATH}/google_use_index.nmslib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_use_index.addDataPointBatch(use_emb)\n",
    "# google_use_index.createIndex({'post': 2}, print_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Google's Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using /tmp/tfhub_modules to cache modules.\n",
      "INFO:tensorflow:Downloading TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/1'.\n",
      "INFO:tensorflow:Downloaded TF-Hub Module 'https://tfhub.dev/google/universal-sentence-encoder/1'.\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_0:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_0\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_1:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_1\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_10:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_10\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_11:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_11\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_12:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_12\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_13:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_13\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_14:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_14\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_15:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_15\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_16:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_16\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_2:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_2\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_3:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_3\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_4:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_4\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_5:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_5\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_6:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_6\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_7:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_7\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_8:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_8\n",
      "INFO:tensorflow:Initialize variable module/Embeddings_en/sharded_9:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Embeddings_en/sharded_9\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_0/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_1/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_2/weights\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/projection:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/projection\n",
      "INFO:tensorflow:Initialize variable module/Encoder_en/DNN/ResidualHidden_3/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with Encoder_en/DNN/ResidualHidden_3/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_1/weights\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/bias\n",
      "INFO:tensorflow:Initialize variable module/SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SHARED_RANK_ANSWER/response_encoder_0/tanh_layer_2/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/LinearLayer/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/LinearLayer/weights\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/bias:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/bias\n",
      "INFO:tensorflow:Initialize variable module/SNLI/Classifier/tanh_layer_0/weights:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with SNLI/Classifier/tanh_layer_0/weights\n",
      "INFO:tensorflow:Initialize variable module/global_step:0 from checkpoint b'/tmp/tfhub_modules/c6f5954ffa065cdb2f2e604e740e8838bf21a2d3/variables/variables' with global_step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    test = session.run(embed(['change the color of the page']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, _ = google_use_index.knnQuery(test, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'change the color of the text on the canvas\\n'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_comment[ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ds/hamel/CodeML/Get_Python_From_BigQuery\n"
     ]
    }
   ],
   "source": [
    "strPATH = str(PATH)\n",
    "print(strPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#google_use_index.saveIndex(f'{strPATH}/google_use_index.nmslib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Index For Fitlam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitlam_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "# fitlam_index.addDataPointBatch(combined_fastailm_emb)\n",
    "# fitlam_index.createIndex({'post': 2}, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitlam_index.saveIndex(f'{strPATH}/fitlam_index.nmslib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ds/hamel/CodeML/Get_Python_From_BigQuery\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery/')\n",
    "strPATH = str(PATH)\n",
    "print(strPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "fitlam_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "fitlam_index.loadIndex(f'{strPATH}/fitlam_index.nmslib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "\n",
    "### Load LangModel Ingredients\n",
    "\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery/')\n",
    "from fastai.text import *\n",
    "\n",
    "trn_lm = np.load(PATH/'trn_lm.npy')\n",
    "val_lm = np.load(PATH/'val_lm.npy')\n",
    "\n",
    "with open(PATH/'lm_itos_dict.pkl', 'rb') as f:\n",
    "    itos = pickle.load(f)\n",
    "\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "PATH=Path('/ds/hamel/fastai/courses/dl1/code_comment_lm')\n",
    "#PATH.mkdir(exist_ok=True)\n",
    "\n",
    "em_sz,nh,nl = 400,400,3\n",
    "wd=1e-7\n",
    "bptt=20\n",
    "bs=32\n",
    "vs=len(itos)\n",
    "trn_dl = LanguageModelLoader(trn_lm, bs, bptt)\n",
    "val_dl = LanguageModelLoader(val_lm, bs, bptt)\n",
    "md = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "learner= md.get_model(opt_fn, em_sz, nh, nl, \n",
    "                        dropouti=drops[0], \n",
    "                        dropout=drops[1], \n",
    "                        wdrop=drops[2], \n",
    "                        dropoute=drops[3], \n",
    "                        dropouth=drops[4])\n",
    "\n",
    "learner.load('code_comment_lm_v3.fai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model From Disk\n",
    "\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery/')\n",
    "lang_model_state_dict = torch.load(PATH/'lm_fastai_codecomment_model_state_dict.pytorch')\n",
    "\n",
    "lang_model = learner.model.cpu()\n",
    "\n",
    "lang_model.reset()\n",
    "lang_model.eval()\n",
    "lang_model.load_state_dict(lang_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2arr(inp):\n",
    "    \"\"\"Convert string to array of dimension (seq_len, 1).\"\"\"\n",
    "    arr = np.expand_dims(np.array([stoi[x] for x in inp.lower().split()]), -1)\n",
    "    return V(T(arr))\n",
    "\n",
    "def str2emb(inp):\n",
    "    \"\"\"Convert string to embedding with lang model\"\"\"\n",
    "    v_arr = str2arr(inp)\n",
    "    lang_model.reset()\n",
    "    hidden_states = lang_model(v_arr)[-1][-1]\n",
    "    lang_model.reset()\n",
    "    return torch.cat([hidden_states.mean(0), \n",
    "                      hidden_states.max(0)[0], \n",
    "                      hidden_states[-1]], \n",
    "                     -1).data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Battle of the sentence encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'tokenize text in parallel and add padding using process based threading'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Customized Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query = str2emb(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbor comment in vector-space to query: \"tokenize text in parallel and add padding using process based threading\" \n",
      "\n",
      " take input text and return tokens w/ part of speech tags using nltk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ids, distances = fitlam_index.knnQuery(test_query, k=1)\n",
    "print(f'Nearest neighbor comment in vector-space to query: \"{query}\" \\n\\n', train_comment[ids[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Google Universal Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    test_query = session.run(embed([query]))\n",
    "    \n",
    "ids, _ = google_use_index.knnQuery(test_query, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nearest neighbor comment in vector-space to query: \"tokenize text in parallel and add padding using process based threading\" \n",
      "\n",
      " wrapper around optimizers to apply gradient processors .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Nearest neighbor comment in vector-space to query: \"{query}\" \\n\\n', train_comment[ids[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE-OF-CONDUCT.md  README.rst\t\t       environment-old.yml  setup.py\r\n",
      "LICENSE\t\t    courses\t\t       environment.yml\t    tests\r\n",
      "MANIFEST\t    docs\t\t       fastai\t\t    tutorials\r\n",
      "MANIFEST.in\t    environment-cpu.yml        requirements.txt\r\n",
      "README.md\t    environment-nopytorch.yml  setup.cfg\r\n"
     ]
    }
   ],
   "source": [
    "!ls /ds/hamel/fastai/fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ln -s /ds/hamel/fastai/fastai fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4978625, 1200)"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery')\n",
    "train_code_to_lang_vecs = np.load(PATH/'train_code_to_lang_vecs.npy')\n",
    "train_code_to_lang_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "demo_index = nmslib.init(method='hnsw', space='cosinesimil')\n",
    "PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery/')\n",
    "strPATH = str(PATH)\n",
    "demo_index.loadIndex(f'{strPATH}/demo_index.nmslib')\n",
    "\n",
    "# demo_index.addDataPointBatch(train_code_to_lang_vecs)\n",
    "# demo_index.createIndex({'post': 2}, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path \n",
    "# PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery/')\n",
    "# strPATH = str(PATH)\n",
    "# demo_index.saveIndex(f'{strPATH}/demo_index.nmslib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_2 = Path('/ds/hohsiangwu/projects/semantic_search')\n",
    "with open(PATH_2/'train.lineage', 'r') as f:\n",
    "    train_lineage = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import regex\n",
    "\n",
    "def print_raw_text(url, ln=0):\n",
    "    \n",
    "    raw_url = url.strip()\n",
    "    linenum = int(regex.search(r'\\#L([0-9]*)$', raw_url).group(1))\n",
    "    filename = regex.search(r'https://github.com/(.*)#L', raw_url).group(1)\n",
    "    filename = regex.sub(r'/blob/', '/', filename)\n",
    "    raw_url = 'https://raw.githubusercontent.com/'+filename\n",
    "    response = requests.get(raw_url)\n",
    "    assert response.status_code == 200\n",
    "    html = response.text \n",
    "    \n",
    "    # Print all the things\n",
    "    print('\\n URL:', url, '\\n------- Code Preview ----------\\n')\n",
    "    print('\\n'.join(html.split('\\n')[linenum-1:linenum+20]))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_valid_url(ids):\n",
    "    for id_ in ids:\n",
    "        url = train_lineage[id_].strip()\n",
    "        response = requests.get(url).status_code\n",
    "        if response == 200:\n",
    "            return url\n",
    "    else:\n",
    "        raise ValueError(f'No valid URLS: {[train_lineage[x] for x in ids]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'stream data from kafka into an array'\n",
    "test_query = str2emb(query)\n",
    "ids, distances = demo_index.knnQuery(test_query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3880852,   22183, 2136290], dtype=int32)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)\n",
    "@register_cell_magic\n",
    "def search(line, cell):\n",
    "    test_query = str2emb(cell)\n",
    "    ids, distances = demo_index.knnQuery(test_query, k=5)\n",
    "    url = get_first_valid_url(ids)\n",
    "    print_raw_text(url)\n",
    "    #print_raw_text(train_lineage[ids[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Code Search Demo\n",
    "\n",
    "Results are live, and are retrieved through machine learning technique described here:\n",
    "https://www.youtube.com/watch?v=nSQIyqtWroU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " URL: https://github.com/Kismuz/btgym/blob/master/btgym/algorithms/nn/networks.py#L152 \n",
      "------- Code Preview ----------\n",
      "\n",
      "def dense_aac_network(x, ac_space, name='dense_aac', linear_layer_ref=noisy_linear, reuse=False):\n",
      "    \"\"\"\n",
      "    Stage3 network: from LSTM flattened output to advantage actor-critic.\n",
      "\n",
      "    Returns:\n",
      "        logits tensor\n",
      "        value function tensor\n",
      "        action sampling function.\n",
      "    \"\"\"\n",
      "    with tf.variable_scope(name, reuse=reuse):\n",
      "        # Center-logits:\n",
      "        logits = norm_layer(\n",
      "            linear_layer_ref(\n",
      "                x=x,\n",
      "                size=ac_space,\n",
      "                name='action',\n",
      "                initializer=normalized_columns_initializer(0.01),\n",
      "                reuse=reuse\n",
      "            ),\n",
      "            center=True,\n",
      "            scale=False,\n"
     ]
    }
   ],
   "source": [
    "%%search\n",
    "\n",
    "keras RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
