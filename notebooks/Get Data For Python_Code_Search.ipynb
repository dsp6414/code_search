{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3485a6eae79e44219e18f2f1737471c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading https://storage.googleapis.com/python_github/000000000000.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000001.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000002.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000003.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000004.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000005.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000006.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000007.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000008.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000009.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000010.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000011.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000012.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000013.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000014.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000015.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000016.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000017.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000018.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000019.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000020.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000021.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000022.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000023.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000024.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000025.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000026.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000027.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000028.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000029.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000030.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000031.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000032.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000033.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000034.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000035.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000036.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000037.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000038.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000039.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000040.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000041.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000042.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000043.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000044.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000045.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000046.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000047.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000048.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000049.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000050.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000051.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000052.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000053.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000054.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000055.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000056.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000057.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000058.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000059.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000060.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000061.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000062.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000063.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000064.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000065.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000066.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000067.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000068.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000069.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000070.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000071.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000072.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000073.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000074.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000075.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000076.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000077.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000078.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000079.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000080.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000081.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000082.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000083.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000084.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000085.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000086.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000087.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000088.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000089.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000090.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000091.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000092.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000093.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000094.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000095.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000096.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000097.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000098.csv\n",
      "reading https://storage.googleapis.com/python_github/000000000099.csv\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_path</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wuyuewen/libcloud libcloud/test/compute/test_v...</td>\n",
       "      <td># Licensed to the Apache Software Foundation (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rnov/Fingerpay work_unit/payfi/apidropbox.py</td>\n",
       "      <td>import os\\nfrom multiprocessing import Pool, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instrat-nigeria/django-instrat-oppia oppia/tem...</td>\n",
       "      <td># oppia/templatetags/display_functions.py\\nimp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kalev/anaconda pyanaconda/installclasses/rhel.py</td>\n",
       "      <td>#\\n# rhel.py\\n#\\n# Copyright (C) 2010  Red Hat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stackforge/watcher watcher/api/controllers/bas...</td>\n",
       "      <td># -*- encoding: utf-8 -*-\\n#\\n# Licensed under...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           repo_path  \\\n",
       "0  wuyuewen/libcloud libcloud/test/compute/test_v...   \n",
       "1       rnov/Fingerpay work_unit/payfi/apidropbox.py   \n",
       "2  instrat-nigeria/django-instrat-oppia oppia/tem...   \n",
       "3   kalev/anaconda pyanaconda/installclasses/rhel.py   \n",
       "4  stackforge/watcher watcher/api/controllers/bas...   \n",
       "\n",
       "                                             content  \n",
       "0  # Licensed to the Apache Software Foundation (...  \n",
       "1  import os\\nfrom multiprocessing import Pool, T...  \n",
       "2  # oppia/templatetags/display_functions.py\\nimp...  \n",
       "3  #\\n# rhel.py\\n#\\n# Copyright (C) 2010  Red Hat...  \n",
       "4  # -*- encoding: utf-8 -*-\\n#\\n# Licensed under...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import ipdb\n",
    "\n",
    "def idx2fname(i):\n",
    "    \"\"\" \n",
    "    Convert integer to filename for downloading python files from \n",
    "    Google Cloud Bucket there are 99 files with suffix 00 - 99.\n",
    "    \"\"\"\n",
    "    suffix = str(100 + i)[-2:]\n",
    "    filename = f'https://storage.googleapis.com/python_github/0000000000{suffix}.csv'\n",
    "    return filename\n",
    "\n",
    "\n",
    "#Download all files and read them into pandas\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for i in tqdm_notebook(range(100)):\n",
    "    filename = idx2fname(i)\n",
    "    print(f'reading {filename}')\n",
    "    dataframes.append(pd.read_csv(filename))\n",
    "\n",
    "# concatenate all files\n",
    "df = pd.concat(dataframes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('python_files_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3409957, 2)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 15G Apr 26 21:04 python_files_df.pkl\r\n"
     ]
    }
   ],
   "source": [
    "! ls -lah python_files_df.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse out (function, comment) pairs w/metadata for lineage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import astor\n",
    "import ast\n",
    "\n",
    "\n",
    "def ast_to_code(ast):\n",
    "    try:\n",
    "        return astor.to_source(ast)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "def get_function_ast(code_blob):\n",
    "    \"\"\"\n",
    "    Given a code blob, return a list of functions.\n",
    "    \n",
    "    This is retrieved from both methods and top-level functions.\n",
    "    \"\"\"\n",
    "    methods = []\n",
    "    try:\n",
    "        mod = ast.parse(code_blob)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    #ipdb.set_trace()\n",
    "    classes = [node for node in mod.body if isinstance(node, ast.ClassDef)]\n",
    "    functions = [node for node in mod.body if isinstance(node, ast.FunctionDef)]\n",
    "    \n",
    "    for c in classes:\n",
    "        methods += [node for node in c.body if isinstance(node, ast.FunctionDef)]\n",
    "    \n",
    "    functions += methods\n",
    "    \n",
    "    # filter to make sure (1) there is a docstring (2) ast can be turned back into code.\n",
    "    return [(ast.get_docstring(f), ast_to_code(f), f.lineno, f.name) for f in functions if ast.get_docstring(f) and ast_to_code(f)]\n",
    "\n",
    "def get_code_comment_pair(code_blob, ref):\n",
    "    funcs = get_function_ast(code_blob)\n",
    "    \n",
    "    if not funcs:\n",
    "        return pd.DataFrame({'ref':[], 'code':[]})\n",
    "    \n",
    "    else:\n",
    "        docstr, code, lineno, func_name = zip(*funcs)\n",
    "        return pd.DataFrame({'ref':ref, \n",
    "                             'docstr':docstr,\n",
    "                             'code': code,\n",
    "                             'line_no':lineno, \n",
    "                             'func_name':func_name,\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_comment_pairs_df(df, code_field='content'):\n",
    "    # basic input checks\n",
    "    assert code_field in df.columns, f'column `{code_field}` not in dataframe'\n",
    "    assert df.index.nunique() == df.shape[0], 'dataframe Index must have unique values'\n",
    "    \n",
    "    #collect code_blob, ref pairs\n",
    "    code_blobs = zip(df[code_field].values, df.index.values)\n",
    "    return pd.concat([get_code_comment_pair(code, ref) for code, ref in code_blobs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathos.multiprocessing import Pool, cpu_count\n",
    "cpu_cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to dataframe\n",
    "\n",
    "#split dataframe into chunks\n",
    "splitdf = np.array_split(df, cpu_cores)\n",
    "\n",
    "pool = Pool(cpu_cores)\n",
    "transformed_data = pool.map(get_code_comment_pairs_df, splitdf)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "final_df = pd.concat(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstr</th>\n",
       "      <th>func_name</th>\n",
       "      <th>line_no</th>\n",
       "      <th>ref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def as_dict(self):\\n    \"\"\"Render this object ...</td>\n",
       "      <td>Render this object as a dict of its fields.</td>\n",
       "      <td>as_dict</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def unset_fields_except(self, except_list=None...</td>\n",
       "      <td>Unset fields so they don't appear in the messa...</td>\n",
       "      <td>unset_fields_except</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def run(self, force=False):\\n    \"\"\"\\n        ...</td>\n",
       "      <td>Runs the daily searcher, queuing selected epis...</td>\n",
       "      <td>run</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def read_from_h5(file_name, **kwargs):\\n    \"\"...</td>\n",
       "      <td>Read data from an H5 file in SXS format\\n\\nNot...</td>\n",
       "      <td>read_from_h5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def write_to_h5(w, file_name, file_write_mode=...</td>\n",
       "      <td>Output the Waveform in NRAR format.\\n\\nNote th...</td>\n",
       "      <td>write_to_h5</td>\n",
       "      <td>244.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def as_dict(self):\\n    \"\"\"Render this object ...   \n",
       "1  def unset_fields_except(self, except_list=None...   \n",
       "0  def run(self, force=False):\\n    \"\"\"\\n        ...   \n",
       "0  def read_from_h5(file_name, **kwargs):\\n    \"\"...   \n",
       "1  def write_to_h5(w, file_name, file_write_mode=...   \n",
       "\n",
       "                                              docstr            func_name  \\\n",
       "0        Render this object as a dict of its fields.              as_dict   \n",
       "1  Unset fields so they don't appear in the messa...  unset_fields_except   \n",
       "0  Runs the daily searcher, queuing selected epis...                  run   \n",
       "0  Read data from an H5 file in SXS format\\n\\nNot...         read_from_h5   \n",
       "1  Output the Waveform in NRAR format.\\n\\nNote th...          write_to_h5   \n",
       "\n",
       "   line_no  ref  \n",
       "0     33.0  4.0  \n",
       "1     40.0  4.0  \n",
       "0     39.0  6.0  \n",
       "0     59.0  9.0  \n",
       "1    244.0  9.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_pickle('parsed_python_code_comment_pairs_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 3.6G Apr 26 21:25 parsed_python_code_comment_pairs_df.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah parsed_python_code_comment_pairs_df.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import textacy_cleaner\n",
    "import re\n",
    "def custom_clean_code(code_string_list):\n",
    "    \"\"\"\n",
    "    Helper function to clean code by performing the following:\n",
    "    1. Insert a space before each uppercase and underscore\n",
    "    2. Apply textacy_cleaner from ktext\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    code_string_list : List[Str]\n",
    "        List of strings that are code blobs\n",
    "    \"\"\"\n",
    "    cleaned_code = []\n",
    "    regex_pattern = r\"([A-Z]|_|\\(|\\)|\\s)\" # find all upper case and underscore characters\n",
    "    \n",
    "    for code_string in code_string_list:\n",
    "        code_fix = re.sub(regex_pattern, r\" \\1\", code_string) #insert a space before\n",
    "        cleaned_code.append(textacy_cleaner(code_fix)) # apply textacy cleaner from ktext\n",
    "    return cleaned_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_code_df(df, code_field='code'):\n",
    "    assert code_field in df.columns, f'column `{code_field}` not in dataframe'\n",
    "    clean_code_list = custom_clean_code(df[code_field].values)\n",
    "    return df.assign(clean_code = clean_code_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function to dataframe\n",
    "\n",
    "#split dataframe into chunks\n",
    "splitdf = np.array_split(final_df, cpu_cores)\n",
    "\n",
    "pool = Pool(cpu_cores)\n",
    "transformed_data = pool.map(clean_code_df, splitdf)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "final_df_clean = pd.concat(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_clean.to_pickle('parsed_python_code_comment_pairs_tokenized_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstr</th>\n",
       "      <th>func_name</th>\n",
       "      <th>line_no</th>\n",
       "      <th>ref</th>\n",
       "      <th>clean_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def as_dict(self):\\n    \"\"\"Render this object ...</td>\n",
       "      <td>Render this object as a dict of its fields.</td>\n",
       "      <td>as_dict</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>def as dict self render this object as a dict ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def unset_fields_except(self, except_list=None...</td>\n",
       "      <td>Unset fields so they don't appear in the messa...</td>\n",
       "      <td>unset_fields_except</td>\n",
       "      <td>40.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>def unset fields except self except list= none...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def run(self, force=False):\\n    \"\"\"\\n        ...</td>\n",
       "      <td>Runs the daily searcher, queuing selected epis...</td>\n",
       "      <td>run</td>\n",
       "      <td>39.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>def run self force= false runs the daily searc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def read_from_h5(file_name, **kwargs):\\n    \"\"...</td>\n",
       "      <td>Read data from an H5 file in SXS format\\n\\nNot...</td>\n",
       "      <td>read_from_h5</td>\n",
       "      <td>59.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>def read from h5 file name kwargs read data fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def write_to_h5(w, file_name, file_write_mode=...</td>\n",
       "      <td>Output the Waveform in NRAR format.\\n\\nNote th...</td>\n",
       "      <td>write_to_h5</td>\n",
       "      <td>244.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>def write to h5 w file name file write mode=w ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def as_dict(self):\\n    \"\"\"Render this object ...   \n",
       "1  def unset_fields_except(self, except_list=None...   \n",
       "0  def run(self, force=False):\\n    \"\"\"\\n        ...   \n",
       "0  def read_from_h5(file_name, **kwargs):\\n    \"\"...   \n",
       "1  def write_to_h5(w, file_name, file_write_mode=...   \n",
       "\n",
       "                                              docstr            func_name  \\\n",
       "0        Render this object as a dict of its fields.              as_dict   \n",
       "1  Unset fields so they don't appear in the messa...  unset_fields_except   \n",
       "0  Runs the daily searcher, queuing selected epis...                  run   \n",
       "0  Read data from an H5 file in SXS format\\n\\nNot...         read_from_h5   \n",
       "1  Output the Waveform in NRAR format.\\n\\nNote th...          write_to_h5   \n",
       "\n",
       "   line_no  ref                                         clean_code  \n",
       "0     33.0  4.0  def as dict self render this object as a dict ...  \n",
       "1     40.0  4.0  def unset fields except self except list= none...  \n",
       "0     39.0  6.0  def run self force= false runs the daily searc...  \n",
       "0     59.0  9.0  def read from h5 file name kwargs read data fr...  \n",
       "1    244.0  9.0  def write to h5 w file name file write mode=w ...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 25G\r\n",
      "drwxr-xr-x 2 root root 6.0K Apr 26 23:49 .\r\n",
      "drwxrwxrwx 7 1001 1001 6.0K Apr 26 23:49 ..\r\n",
      "-rw-r--r-- 1 root root 3.6G Apr 26 21:25 parsed_python_code_comment_pairs_df.pkl\r\n",
      "-rw-r--r-- 1 root root 5.8G Apr 26 23:49 parsed_python_code_comment_pairs_tokenized_df.pkl\r\n",
      "-rw-r--r-- 1 root root  15G Apr 26 21:04 python_files_df.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah /ds/CodeML/Get_Python_From_BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_df_clean = pd.read_pickle('parsed_python_code_comment_pairs_tokenized_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize For Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#train_df, holdout_df = train_test_split(final_df_clean, train_size=.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_pickle('final_train_df.pkl')\n",
    "holdout_df.to_pickle('final_holdout_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4023138, 6), (1005785, 6))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, holdout_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_code = train_df.code.values.tolist()\n",
    "train_comment = train_df.docstr.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 50 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 326 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 199 sec\n",
      "WARNING:root:Finished parsing 4,023,138 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 149 sec\n",
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:Setting maximum document length to 20 based upon hueristic of 0.7 percentile.\n",
      " See full histogram by insepecting the `document_length_stats` attribute.\n",
      "WARNING:root:(1/2) done. 175 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 73 sec\n",
      "WARNING:root:Finished parsing 4,023,138 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 87 sec\n"
     ]
    }
   ],
   "source": [
    "from ktext.preprocess import processor\n",
    "code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "comment_proc = processor(append_indicators=True, hueristic_pct_padding=.7, keep_n=14000, padding ='post')\n",
    "t_comment = comment_proc.fit_transform(train_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('py_code_proc.dpkl', 'wb') as f:\n",
    "    dpickle.dump(code_proc, f)\n",
    "\n",
    "with open('py_comment_proc.dpkl', 'wb') as f:\n",
    "    dpickle.dump(comment_proc, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('py_t_code_vecs.npy', t_code)\n",
    "np.save('py_t_comment_vecs.npy', t_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ds/hamel/CodeML/Get_Python_From_BigQuery\r\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
