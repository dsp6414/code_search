{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_pairs_data import read_data\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import dill as dpickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from ktext.preprocess import processor\n",
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor\n",
    "%matplotlib inline\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "USE_CACHE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is retrived through [GH Archive](https://www.gharchive.org/) an open source project that archives public Github code on BigQuery.  This demo uses python top-level functions only, so the first step is to query this data.  Below is the SQL Query used:\n",
    "\n",
    "```{sql}\n",
    "SELECT \n",
    " max(concat(f.repo_name, ' ', f.path)) as repo_path,\n",
    " c.content\n",
    "FROM `bigquery-public-data.github_repos.files` as f\n",
    "JOIN `bigquery-public-data.github_repos.contents` as c on f.id = c.id\n",
    "WHERE \n",
    "  f.path like '%.py' and --with python extension\n",
    "  c.size < 15000 and --get rid of ridiculously long files\n",
    "  REGEXP_CONTAINS(c.content, r'def ') --contains function\n",
    "group by c.content\n",
    "```\n",
    "\n",
    "You can also view this query directly on Bigquery with the following URL: https://bigquery.cloud.google.com/savedquery/506213277345:c5e99f7fd4a04c67814eb7e992b49f6d\n",
    "\n",
    "For convienience, I have cached the results of this query into a pickled dataframe hosted on Google Cloud: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CACHE:\n",
    "    df = pd.read_csv('...')\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Docstrings And Functions\n",
    "\n",
    "We will take this data and write out three types of text files:\n",
    "\n",
    "1. {train/valid/test}.function  : each line contains function tokens seperated by spaces\n",
    "2. {train/valid/test}.docstring : each line contains docstring tokens seperated by spaces\n",
    "3. {train/valid/test}.lineage   : each line contains a url link to the original function\n",
    "\n",
    "All of these files have the same number of rows because each row is related to eachother.  The training data only includes top-level functions that have docstrings.  **A an important TODO is to use the code without any docstrings as a holdout set and run search on that. **\n",
    "\n",
    "The parsing of code is done via python's built in `AST` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CACHE:\n",
    "    #TODO: Ho-Hsiang add your code here\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_code rows: 4,978,625\n",
      "holdout_code rows: 50,290\n",
      "total code rows: 5,028,915\n",
      "\n",
      "train_comment rows: 4,978,625\n",
      "holdout_comment rows: 50,290\n",
      "total comment rows: 5,028,915\n",
      "\n",
      "train_lineage rows: 4,978,625\n",
      "holdout_comment rows: 50,290\n",
      "total lineage rows: 5,028,915\n"
     ]
    }
   ],
   "source": [
    "if USE_CACHE:\n",
    "\n",
    "    PATH = Path('/ds/hohsiangwu/projects/semantic_search')\n",
    "\n",
    "    train_code, train_comment, holdout_code, holdout_comment, train_lineage, holdout_lineage = \\\n",
    "    read_data(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data With `Ktext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CACHE:\n",
    "    \n",
    "    \n",
    "    code_proc = processor(hueristic_pct_padding=.7, keep_n=20000)\n",
    "    t_code = code_proc.fit_transform(train_code)\n",
    "\n",
    "    comment_proc = processor(append_indicators=True, \n",
    "                             hueristic_pct_padding=.7, \n",
    "                             keep_n=15000, \n",
    "                             padding ='post')\n",
    "    \n",
    "    t_comment = comment_proc.fit_transform(train_comment)\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    # TODO: finish this\n",
    "    \n",
    "    #Save the preprocessor\n",
    "    with open('py_code_proc.dpkl', 'wb') as f:\n",
    "        dpickle.dump(code_proc, f)\n",
    "\n",
    "    with open('py_comment_proc.dpkl', 'wb') as f:\n",
    "        dpickle.dump(comment_proc, f)\n",
    "\n",
    "    # # Save the processed data\n",
    "    np.save('py_t_code_vecs.npy', t_code)\n",
    "    np.save('py_t_comment_vecs.npy', t_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (4978625, 45)\n",
      "Shape of decoder input: (4978625, 14)\n",
      "Shape of decoder target: (4978625, 14)\n",
      "Size of vocabulary for /ds/hohsiangwu/projects/semantic_search/py_code_proc.dpkl: 10,002\n",
      "Size of vocabulary for /ds/hohsiangwu/projects/semantic_search/py_comment_proc.dpkl: 8,002\n"
     ]
    }
   ],
   "source": [
    "if USE_CACHE:\n",
    "    encoder_input_data, doc_length = load_encoder_inputs(PATH/'py_t_code_vecs.npy')\n",
    "    decoder_input_data, decoder_target_data = load_decoder_inputs(PATH/'py_t_comment_vecs.npy')\n",
    "    num_encoder_tokens, body_pp = load_text_processor(PATH/'py_code_proc.dpkl')\n",
    "    num_decoder_tokens, title_pp = load_text_processor(PATH/'py_comment_proc.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Function Summarizer So You Can Use This For Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 800)    6401600     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 45)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 800)    3200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 800)          11847200    Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 800),  3842400     Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 800)    3200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 8002)   6409602     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 28,507,202\n",
      "Trainable params: 28,502,402\n",
      "Non-trainable params: 4,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 800\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "# x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "# x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU', dropout=.5)(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU', dropout=.5)\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "seq2seq_Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for /ds/hamel/CodeML/Get_Python_From_BigQuery/py_code_proc.dpkl: 20,002\n",
      "Size of vocabulary for /ds/hamel/CodeML/Get_Python_From_BigQuery/py_comment_proc.dpkl: 15,002\n"
     ]
    }
   ],
   "source": [
    "if not USE_CACHE:\n",
    "    # Train Model\n",
    "    seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.0015), loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    script_name_base = 'py_func_sum_v2_'\n",
    "    csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "    model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                       save_best_only=True)\n",
    "\n",
    "    batch_size = 900\n",
    "    epochs = 60\n",
    "    history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=0.12, callbacks=[csv_logger, model_checkpoint])\n",
    "\n",
    "if USE_CACHE:\n",
    "    PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery')\n",
    "\n",
    "    num_encoder_tokens, body_pp = load_text_processor(PATH/'py_code_proc.dpkl')\n",
    "    num_decoder_tokens, title_pp = load_text_processor(PATH/'py_comment_proc.dpkl')\n",
    "    seq2seq_Model = load_model(PATH/'seq2seq_code_search_py_v2.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check Predictions of Function Summarizer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 8562 =================\n",
      "\n",
      "Issue Body:\n",
      " def fit self X y None self random_state check_random_state self random_state X np asarray X code_init self V_init T if self V_init is not None else None dict_init self U_init T if self U_init is not None else None Vt _ E dict_learning X T self n_components self alpha tol self tol max_iter self max_iter method self method n_jobs self n_jobs verbose self verbose random_state self random_state code_init code_init dict_init dict_init self components_ Vt T self error_ E return self\n",
      " \n",
      "\n",
      "Original Title:\n",
      " fit the model from data in x.\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " fit the model from data in x\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 39491 =================\n",
      "\n",
      "Issue Body:\n",
      " def test_nestedClass self self flakes def f foo class C bar foo def f self return foo return C f 123 f\n",
      " \n",
      "\n",
      "Original Title:\n",
      " nested classes can access enclosing scope\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " use a function to use a nested inside of cdata\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 9134 =================\n",
      "\n",
      "Issue Body:\n",
      " def __init__ self total completed details self total total self completed completed self details details\n",
      " \n",
      "\n",
      "Original Title:\n",
      " : ivar total : the total work units . : type total : int : ivar completed : the completed work units . : type completed : int : ivar details : the reported details . : type details : object\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " initializes the progress bar\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 31666 =================\n",
      "\n",
      "Issue Body:\n",
      " def test_status_no_kumascript client mock_status_externals mock_status_externals kumascript side_effect Requests_ConnectionError Nope url reverse health status response client get url data json loads response content assert data services kumascript available False revision None\n",
      " \n",
      "\n",
      "Original Title:\n",
      " the status json shows if kumascript is unavailable .\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " the status of the status of the server is unavailable\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 49382 =================\n",
      "\n",
      "Issue Body:\n",
      " def stop self pass\n",
      " \n",
      "\n",
      "Original Title:\n",
      " qmovie.stop ( )\n",
      "\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " stop the plugin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)\n",
    "                                 \n",
    "demo_testdf = pd.DataFrame({'body':holdout_code, 'issue_title':holdout_comment, 'issue_url':''})\n",
    "seq2seq_inf.demo_model_predictions(n=5, issue_df=demo_testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Fast.AI Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from language_model_utils import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CACHE:\n",
    "    lang_model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CACHE:\n",
    "    PATH = Path('/ds/hamel/CodeML/Get_Python_From_BigQuery/')\n",
    "    lang_model = torch.load(PATH/'lm_fastai_codecomment_model.pytorch')\n",
    "    lang_model.eval()\n",
    "    lang_model.reset()\n",
    "    \n",
    "    # Load Fastai Embeddings\n",
    "    fastailm_emb = np.load(PATH/'combined_fastailm_emb.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Language Model For Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2arr(inp):\n",
    "    \"\"\"Convert string to array of dimension (seq_len, 1).\"\"\"\n",
    "    arr = np.expand_dims(np.array([stoi[x] for x in inp.lower().split()]), -1)\n",
    "    return V(T(arr))\n",
    "\n",
    "def str2emb(inp):\n",
    "    \"\"\"Convert string to embedding with lang model\"\"\"\n",
    "    v_arr = str2arr(inp)\n",
    "    lang_model.reset()\n",
    "    hidden_states = lang_model(v_arr)[-1][-1]\n",
    "    lang_model.reset()\n",
    "    return torch.cat([hidden_states.mean(0), \n",
    "                      hidden_states.max(0)[0], \n",
    "                      hidden_states[-1]], \n",
    "                     -1).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Locations\n",
    "\n",
    "### Important Files in `/ds/hohsiangwu/projects/semantic_search`\n",
    "\n",
    "1. `{train, valid, test}.function`:   text file, each line is a tokenized function \n",
    "\n",
    "         - train + valid function rows: 4,978,625\n",
    "         - test function rows: 50,290\n",
    "         - total function rows: 5,028,915\n",
    "\n",
    "\n",
    "2. `{train, valid, test}.docstring`:  text file, each line is a tokenized docstring\n",
    "\n",
    "         - train + valid comment rows: 4,978,625\n",
    "         - test comment rows: 50,290\n",
    "         - total comment rows: 5,028,915\n",
    "\n",
    "\n",
    "### Important Files in `/ds/hamel/CodeML/Get_Python_From_BigQuery/`\n",
    "\n",
    "1.  `use_emb.npy`            :          Google Universal Sentence Encoder.  shape: (4978625, 512)\n",
    "\n",
    "2. `concat_train_avg_emb.npy`:          language model average pooling.     shape: (4978625, 400)\n",
    "\n",
    "3. `concat_train_max_emb.npy`:          language model max pooling.         shape: (4978625, 400)\n",
    "\n",
    "4. `concat_train_last_emb.npy`:         language model last hidden state.   shape: (4978625, 400)\n",
    "\n",
    "5. `combined_fastailm_emb`    :         Horizontal concat of [2, 3, 4].     shape: (4978625, 1200)\n",
    "\n",
    "6.  `codeSearch_Model_frozen.hdf5`:     My best keras model with val_loss = -0.8061 cosine proximity loss.   \n",
    "\n",
    "```\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "\n",
    "Encoder-Input (InputLayer)   (None, 55)                0         \n",
    "_________________________________________________________________\n",
    "Encoder-Model (Model)        (None, 800)               19847200  \n",
    "_________________________________________________________________\n",
    "dense_3 (Dense)              (None, 1024)              820224    \n",
    "_________________________________________________________________\n",
    "bn-1 (BatchNormalization)    (None, 1024)              4096      \n",
    "_________________________________________________________________\n",
    "dense_4 (Dense)              (None, 1200)              1230000   \n",
    "\n",
    "\n",
    "Total params: 21,901,520\n",
    "Trainable params: 2,052,272\n",
    "Non-trainable params: 19,849,248\n",
    "```\n",
    "7. `lm_fastai_codecomment_model.pytorch`:  This is the language model trained with fastai, you will have to have the latest version of the fast.ai library\n",
    "\n",
    "8.  `lm_fastai_codecomment_model_state_dict`: This is the state dict of the language model, a more lightweight way of re-instantiating the model's parameters.\n",
    "\n",
    "9.  `fitlam_index.nmslib`:  This is the nmslib index that has all the code after it has been vectorized by the language model.  \n",
    "\n",
    "### Important Notebooks\n",
    "\n",
    "1.  `hamel/CodeML/Get_Python_From_BigQuery/Parse_Ho_Hsian_Files.ipynb`  this notebook where I do things that are CPU intense:\n",
    " - preprocess the {training, validation, lineage} files. \n",
    " - run all the comments through the vectorizers (did a version for both Google and My own languagel model).\n",
    " - Loaded all the vectors into an NMS Lib\n",
    " - **This is the notebook where the actual demo lives**\n",
    "\n",
    "2. `hamel/fastai/courses/dl1/lang-model-code-comments.ipynb`:  this notebook is where i trained the fastai language model, and also where I then used the trained model to vectorize all the comments.\n",
    "\n",
    "3. `/hamel/CodeML/projects/function_summarizer/keras-code-search.ipynb` this is the notebook where I\n",
    " - train a function summarizer \n",
    " - fine tune the function summarizer to predict embedding instead of docstring\n",
    " - make predictions for all the training data to vectorize all the code (happens on GPU).  This is the code that is loaded into the index where you want to do nearest neighbor search from.\n",
    "\n",
    "4. `hamel/CodeML/Get_Python_From_BigQuery/Get%20Data%20For%20Python_Code_Search.ipynb` - this notebook I was using to build my own training set, but I abandoned this in favor of Ho-Hsiang's Training dataset which he gave me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
